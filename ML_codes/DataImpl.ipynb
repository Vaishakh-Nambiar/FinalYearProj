{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BBtxSS_LzqpH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trials:"
      ],
      "metadata": {
        "id": "BBtxSS_LzqpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "yuZd5XLu2vv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4C7GqTS2qhl",
        "outputId": "fce26668-ef94-4324-d152-f71c13c53e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   26.84    -2.68    23.09 -1014.33]\n",
            " [   30.19   -23.72    23.3  -1011.56]\n",
            " [   29.48     1.24    19.01 -1012.23]\n",
            " ...\n",
            " [   28.08    -8.38    31.9  -1024.12]\n",
            " [   27.01    -1.48    19.46 -1019.94]\n",
            " [   25.69   -23.31    26.69 -1028.95]]\n",
            "------------------------\n",
            "[26.84 30.19 29.48 ... 28.08 27.01 25.69]\n"
          ]
        }
      ],
      "source": [
        "# Load your dataset (replace 'your_dataset.csv' with your actual dataset)\n",
        "# Assuming your dataset has columns: 'Temperature', 'Vibration_X', 'Vibration_Y', 'Vibration_Z', and 'Target'\n",
        "# dataset = pd.read_csv(\"data_1.csv\")\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/V Sem7/End-Sem Project/Data/Material 1/data_1.csv\")\n",
        "\n",
        "# Extract features and target variable\n",
        "X = dataset[[\"Temperature\", \"Vibration_X\", \"Vibration_Y\", \"Vibration_Z\"]].values\n",
        "\n",
        "#  Confusion here is with the 'target' value... letss see\n",
        "# y = dataset[\"Target\"].values\n",
        "y = dataset[\"Temperature\"].values\n",
        "\n",
        "\n",
        "print(X)\n",
        "print('------------------------')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "x_hVk4oI20eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_3d = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n"
      ],
      "metadata": {
        "id": "z_3wWbvj2520"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR4ZEbeQ8uV_",
        "outputId": "91b787eb-a723-4810-e4ce-a2ca4b2f3c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.78473091, 0.06742323, 0.50016683, 0.04126984]],\n",
              "\n",
              "       [[0.17897372, 0.06909212, 0.75008342, 0.2037037 ]],\n",
              "\n",
              "       [[0.79474343, 0.6975968 , 0.00734067, 0.31693122]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.89111389, 0.3411215 , 0.95762429, 0.12010582]],\n",
              "\n",
              "       [[0.54443054, 0.34445928, 0.67600934, 0.61851852]],\n",
              "\n",
              "       [[0.61451815, 0.12917223, 0.40707374, 0.02433862]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(50, input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
        "model_lstm.add(Dense(1))\n",
        "model_lstm.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n"
      ],
      "metadata": {
        "id": "q8EK9Lym5dos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the LSTM model\n",
        "model_lstm.fit(X_train_3d, y_train, epochs=100, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6z1LZlq6ke1",
        "outputId": "bd5b2f7c-f6be-48b2-e85e-1122af3568d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 2s 2ms/step - loss: 843.5424\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 827.1457\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 801.6179\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 761.3638\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 704.5815\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 633.7591\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 554.7485\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 473.8095\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 397.0703\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 327.4947\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 266.7822\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 215.1898\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 172.0419\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 136.5888\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 107.8799\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 84.7200\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 66.4706\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 52.0620\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 40.9604\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 32.3954\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 25.9088\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 21.0226\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 17.4112\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 14.7539\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 12.8139\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 11.4077\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 10.4155\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 9.7010\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 9.1808\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 8.8077\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 8.5436\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 8.3319\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 8.1714\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 8.0477\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 7.9349\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 7.8381\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 7.7510\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 7.6697\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 7.5882\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 7.5120\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 7.4380\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 7.3595\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 7.2842\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 7.2098\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 7.1343\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 7.0597\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.9879\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.9135\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 6.8374\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.7646\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.6909\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 6.6170\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.5443\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 6.4709\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.3997\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.3269\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 6.2560\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.1839\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 6.1128\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 6.0430\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 5.9722\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 5.9022\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 5.8342\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 5.7635\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 5.6959\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 5.6265\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 5.5583\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 5.4905\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 5.4236\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 5.3580\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 5.2900\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 5.2243\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 5.1619\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 5.0955\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 5.0289\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 4.9633\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 4.8997\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4.8378\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4.7735\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4.7101\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.6466\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.5851\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.5240\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.4616\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.4007\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.3403\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.2805\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4.2187\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.1589\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.0981\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 4.0394\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.9796\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.9219\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.8609\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.8038\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.7447\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.6878\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.6283\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.5700\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 3.5131\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f74dbabe0b0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the trained model (adjust X_test accordingly)\n",
        "X_test_3d = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "y_pred = model_lstm.predict(X_test_3d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtaG_jRW2-oc",
        "outputId": "482aa30b-4710-4962-f688-e036b902ade3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model (use appropriate evaluation metrics based on your problem)\n",
        "loss = model_lstm.evaluate(X_test_3d, y_test)\n",
        "print(f\"Test Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsZPExJ87Pyd",
        "outputId": "fdcc63fa-938c-47f4-bea8-54ee78a32a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 3ms/step - loss: 3.2118\n",
            "Test Loss: 3.2117719650268555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the Transformer model\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, num_blocks, input_length):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_blocks = num_blocks\n",
        "        self.input_length = input_length\n",
        "\n",
        "        self.embedding = layers.Embedding(input_dim=input_length, output_dim=embed_dim)\n",
        "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_blocks)]\n",
        "        self.reshape = layers.Reshape((-1, embed_dim))\n",
        "        self.fc = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.reshape(x)\n",
        "\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.transformer_blocks[i](x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the TransformerBlock layer\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Set hyperparameters\n",
        "embed_dim = 32  # Embedding dimension\n",
        "num_heads = 2   # Number of attention heads\n",
        "ff_dim = 32     # Hidden layer size in feedforward network inside transformer\n",
        "num_blocks = 4  # Number of transformer blocks\n",
        "input_length = 50  # Assuming your input sequence length\n",
        "\n",
        "# Instantiate the model\n",
        "model_transformer = TransformerModel(embed_dim, num_heads, ff_dim, num_blocks, input_length)\n",
        "\n",
        "# Compile the model\n",
        "model_transformer.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mean_squared_error\")\n",
        "\n",
        "model_transformer.build((None, input_length))\n",
        "\n",
        "# Display the model summary\n",
        "model_transformer.summary()\n",
        "\n",
        "# Train the model (assuming you have X_train and y_train)\n",
        "# model_transformer.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KELeNIR-zPH",
        "outputId": "9a1a4a64-03c0-44e7-d96b-c1b6255c0586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  1600      \n",
            "                                                                 \n",
            " transformer_block_8 (Trans  multiple                  10656     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_9 (Trans  multiple                  10656     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_10 (Tran  multiple                  10656     \n",
            " sformerBlock)                                                   \n",
            "                                                                 \n",
            " transformer_block_11 (Tran  multiple                  10656     \n",
            " sformerBlock)                                                   \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         multiple                  0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            multiple                  33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44257 (172.88 KB)\n",
            "Trainable params: 44257 (172.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_test is your test input data\n",
        "X_test = X_test_3d\n",
        "\n",
        "# Convert X_test to numpy array\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "# Add an extra dimension to match the model input shape\n",
        "X_test = np.expand_dims(X_test, axis=0)\n",
        "\n",
        "# Make predictions using the Transformer model\n",
        "predictions = model_transformer.predict(X_test)\n",
        "\n",
        "# Display the predictions\n",
        "print(\"Predictions:\")\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "aALW-LNr-zKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have a DataFrame with temperature data for the past 10 days\n",
        "# Replace this with your actual DataFrame or data source\n",
        "data = {\n",
        "    'Day': np.arange(1, 11),\n",
        "    'Temperature': [20.5, 21.0, 22.5, 23.0, 22.8, 22.7, 22.0, 21.5, 21.2, 20.8]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df[['Day']]\n",
        "y = df['Temperature']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the temperature on the 11th day\n",
        "day_11_prediction = model.predict([[11]])\n",
        "\n",
        "print(f\"Predicted Temperature on the 11th day: {day_11_prediction[0]}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
        "\n",
        "# Plot the regression line\n",
        "plt.scatter(X_test, y_test, color='black')\n",
        "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Temperature')\n",
        "plt.title('Linear Regression Model')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "fQh1BpaGA9t-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "1930dcb1-f5c3-40d8-8c46-2ca6e236a5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Temperature on the 11th day: 21.718965517241383\n",
            "Mean Squared Error on Test Set: 0.8347577288941795\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming you have a DataFrame with vibration and temperature data for the past 10 days\n",
        "# Replace this with your actual DataFrame or data source\n",
        "# Here, 'Vibration_X', 'Vibration_Y', 'Vibration_Z' are the vibration features\n",
        "# 'Temperature' is the target variable\n",
        "data = {\n",
        "    'Vibration_X': np.random.rand(1200),\n",
        "    'Vibration_Y': np.random.rand(1200),\n",
        "    'Vibration_Z': np.random.rand(1200),\n",
        "    'Temperature': np.random.rand(1200)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df[['Vibration_X', 'Vibration_Y', 'Vibration_Z']]\n",
        "y = df['Temperature']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a multivariate linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the temperature on the 11th day based on vibration data\n",
        "# Replace this with the actual vibration data for the 11th day\n",
        "vibration_11th_day = np.random.rand(1, 3)  # Assuming 3 features for vibration\n",
        "temperature_11th_day_prediction = model.predict(vibration_11th_day)\n",
        "\n",
        "print(f\"Predicted Temperature on the 11th day: {temperature_11th_day_prediction[0]}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mL1f7ngtNzP",
        "outputId": "3469b851-1d92-49f7-87ce-34635d7d32e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Temperature on the 11th day: 0.47941646923128584\n",
            "Mean Squared Error on Test Set: 0.07562608871647723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming you have a DataFrame with vibration and temperature data for the past 10 days\n",
        "# Replace this with your actual DataFrame or data source\n",
        "# Here, 'Vibration_X', 'Vibration_Y', 'Vibration_Z' are the vibration features\n",
        "# 'Temperature' is the target variable\n",
        "data = {\n",
        "    'Temperature': [26.84, 30.19, 29.48, 31.76, 25.93, 27.45, 31.92, 31.71, 31.83, 29.37, 28.07, 26.27, 26.96, 28.39, 31.53],\n",
        "    'Vibration_X': [-2.68, -23.72, 1.24, -13.17, -13.71, -2.78, -7.38, -0.67, -11.12, -11.81, -18.92, -22.18, -11.35, -0.69, 3.73],\n",
        "    'Vibration_Y': [23.09, 23.3, 19.01, 21.69, 21.47, 27.26, 24.9, 5.46, 32.04, 19.66, 29.78, 14.3, 14.83, 17.6, 12.25],\n",
        "    'Vibration_Z': [-1014.33, -1011.56, -1012.23, -1018.05, -1016.73, -1025.36, -1024.68, -1018.47, -1016.76, -1028.73, -1026.08, -1020.9, -1019.04, -1021.89, -1021.73]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the data into features (X) and target variables (y_vibration, y_temperature)\n",
        "X = df[['Vibration_X', 'Vibration_Y', 'Vibration_Z']]\n",
        "y_vibration = X  # Assuming vibration values are similar to the input features\n",
        "y_temperature = df['Temperature']\n",
        "\n",
        "# Create separate models for vibration and temperature\n",
        "model_vibration = LinearRegression()\n",
        "model_temperature = LinearRegression()\n",
        "\n",
        "# Train the models\n",
        "model_vibration.fit(X, y_vibration)\n",
        "model_temperature.fit(X, y_temperature)\n",
        "\n",
        "# Predict the vibration and temperature for the 11th day based on the trained models\n",
        "# Do not provide actual data for the 11th day\n",
        "vibration_11th_day_prediction = model_vibration.predict(X.iloc[[-1]])  # Use the features of the last available day\n",
        "temperature_11th_day_prediction = model_temperature.predict(X.iloc[[-1]])  # Use the features of the last available day\n",
        "\n",
        "print(f\"Predicted Vibration on the 11th day: {vibration_11th_day_prediction[0]}\")\n",
        "print(f\"Predicted Temperature on the 11th day: {temperature_11th_day_prediction[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpWr9-pNvAl5",
        "outputId": "452704a7-9506-47f4-fdfa-af4e6506cba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Vibration on the 11th day: [    3.73    12.25 -1021.73]\n",
            "Predicted Temperature on the 11th day: 29.870298427069844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming you have a DataFrame with vibration and temperature data for the past 10 days\n",
        "# Replace this with your actual DataFrame or data source\n",
        "# Here, 'Vibration_X', 'Vibration_Y', 'Vibration_Z' are the vibration features\n",
        "# 'Temperature' is the target variable\n",
        "data = {\n",
        "    'Temperature': [26.84, 30.19, 29.48, 31.76, 25.93, 27.45, 31.92, 31.71, 31.83, 29.37, 28.07, 26.27, 26.96, 28.39, 31.53],\n",
        "    'Vibration_X': [-2.68, -23.72, 1.24, -13.17, -13.71, -2.78, -7.38, -0.67, -11.12, -11.81, -18.92, -22.18, -11.35, -0.69, 3.73],\n",
        "    'Vibration_Y': [23.09, 23.3, 19.01, 21.69, 21.47, 27.26, 24.9, 5.46, 32.04, 19.66, 29.78, 14.3, 14.83, 17.6, 12.25],\n",
        "    'Vibration_Z': [-1014.33, -1011.56, -1012.23, -1018.05, -1016.73, -1025.36, -1024.68, -1018.47, -1016.76, -1028.73, -1026.08, -1020.9, -1019.04, -1021.89, -1021.73]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the data into features (X) and target variables (y_vibration, y_temperature)\n",
        "X = df[['Vibration_X', 'Vibration_Y', 'Vibration_Z']]\n",
        "y_vibration = X  # Assuming vibration values are similar to the input features\n",
        "y_temperature = df['Temperature']\n",
        "\n",
        "# Create separate models for vibration and temperature\n",
        "model_vibration = LinearRegression()\n",
        "model_temperature = LinearRegression()\n",
        "\n",
        "# Train the models\n",
        "model_vibration.fit(X, y_vibration)\n",
        "model_temperature.fit(X, y_temperature)\n",
        "\n",
        "# Predict the vibration and temperature for the 11th day based on the trained models\n",
        "# Do not provide actual data for the 11th day\n",
        "vibration_11th_day_prediction = model_vibration.predict(X.iloc[[-1]])  # Use the features of the last available day\n",
        "temperature_11th_day_prediction = model_temperature.predict(X.iloc[[-1]])  # Use the features of the last available day\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-2.51,5,-1021]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 31.51  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error = np.abs(vibration_11th_day_prediction[0] - actual_vibration_11th_day)\n",
        "temperature_error = np.abs(temperature_11th_day_prediction[0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"Predicted Vibration on the 11th day:\", vibration_11th_day_prediction[0])\n",
        "print(\"Vibration Error for the 11th day:\", vibration_error)\n",
        "\n",
        "print(\"\\nActual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"Predicted Temperature on the 11th day:\", temperature_11th_day_prediction[0])\n",
        "print(\"Temperature Error for the 11th day:\", temperature_error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1QlxMeqwxWF",
        "outputId": "667a473f-2f8c-400b-cdbf-00798ee21f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Vibration on the 11th day: [[   -2.51     5.   -1021.  ]]\n",
            "Predicted Vibration on the 11th day: [    3.73    12.25 -1021.73]\n",
            "Vibration Error for the 11th day: [[6.24 7.25 0.73]]\n",
            "\n",
            "Actual Temperature on the 11th day: 31.51\n",
            "Predicted Temperature on the 11th day: 29.870298427069844\n",
            "Temperature Error for the 11th day: 1.6397015729301572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "1UzcIVjAzyb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying logic:"
      ],
      "metadata": {
        "id": "Gp50EJ41z0VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now give me codes, like the one you gave above, for each of the methods that you listed\n",
        "# Where it takes data of 10 days, predict values for the 11th day based on the patterns it learned from the previous days, and we should not provide the actual data for the 11th day as input. Instead, use the features of the previous days and let the model predict the values.\n",
        "# I wanna predict both the vibration and temperature of the 11th day\n",
        "# Also fit a code such that it compares the actual data, and the data it predicted.\n",
        "# And make it as separate code snippets\n",
        "\n",
        "# The dataset sample:\n",
        "# Temperature\tVibration_X\tVibration_Y\tVibration_Z\n",
        "# 26.84\t-2.68\t23.09\t-1014.33\n",
        "# 30.19\t-23.72\t23.3\t-1011.56\n",
        "# 29.48\t1.24\t19.01\t-1012.23\n",
        "# 31.76\t-13.17\t21.69\t-1018.05\n",
        "# 25.93\t-13.71\t21.47\t-1016.73\n",
        "# 27.45\t-2.78\t27.26\t-1025.36\n",
        "# 31.92\t-7.38\t24.9\t-1024.68\n",
        "# 31.71\t-0.67\t5.46\t-1018.47\n",
        "# 31.83\t-11.12\t32.04\t-1016.76\n",
        "# 29.37\t-11.81\t19.66\t-1028.73\n",
        "# 28.07\t-18.92\t29.78\t-1026.08\n",
        "# 26.27\t-22.18\t14.3\t-1020.9\n",
        "# 26.96\t-11.35\t14.83\t-1019.04\n",
        "# 28.39\t-0.69\t17.6\t-1021.89\n",
        "# 31.53\t3.73\t12.25\t-1021.73\n",
        "\n",
        "\n",
        "# Now give the codes accordingly"
      ],
      "metadata": {
        "id": "6UlV94YRyl1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "data = {\n",
        "    'Temperature': [26.84, 30.19, 29.48, 31.76, 25.93, 27.45, 31.92, 31.71, 31.83, 29.37, 28.07, 26.27, 26.96, 28.39, 31.53],\n",
        "    'Vibration_X': [-2.68, -23.72, 1.24, -13.17, -13.71, -2.78, -7.38, -0.67, -11.12, -11.81, -18.92, -22.18, -11.35, -0.69, 3.73],\n",
        "    'Vibration_Y': [23.09, 23.3, 19.01, 21.69, 21.47, 27.26, 24.9, 5.46, 32.04, 19.66, 29.78, 14.3, 14.83, 17.6, 12.25],\n",
        "    'Vibration_Z': [-1014.33, -1011.56, -1012.23, -1018.05, -1016.73, -1025.36, -1024.68, -1018.47, -1016.76, -1028.73, -1026.08, -1020.9, -1019.04, -1021.89, -1021.73]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "QCgp590ZyHeH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (X) and target variables (y_vibration, y_temperature)\n",
        "X = df[['Vibration_X', 'Vibration_Y', 'Vibration_Z']]\n",
        "y_vibration = X  # Assuming vibration values are similar to the input features\n",
        "y_temperature = df['Temperature']\n"
      ],
      "metadata": {
        "id": "ORvsexP-ytpI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create linear regression models\n",
        "model_vibration = LinearRegression()\n",
        "model_temperature = LinearRegression()\n",
        "\n",
        "# Train the models\n",
        "model_vibration.fit(X, y_vibration)\n",
        "model_temperature.fit(X, y_temperature)\n",
        "\n",
        "# Predict the 11th day values\n",
        "vibration_11th_day_prediction = model_vibration.predict(X.iloc[[-1]])\n",
        "temperature_11th_day_prediction = model_temperature.predict(X.iloc[[-1]])\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-2.51,5,-1021]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 31.5  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error = np.abs(vibration_11th_day_prediction[0] - actual_vibration_11th_day)\n",
        "temperature_error = np.abs(temperature_11th_day_prediction[0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"Linear Regression - Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"Linear Regression - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction[0])\n",
        "print(\"Linear Regression - Vibration Error for the 11th day:\", vibration_error)\n",
        "\n",
        "print(\"\\nLinear Regression - Actual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"Linear Regression - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction[0])\n",
        "print(\"Linear Regression - Temperature Error for the 11th day:\", temperature_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9LXg3LyyI0D",
        "outputId": "7219e544-a56b-4178-c92f-0a7010c97d3c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression - Actual Vibration on the 11th day: [[   -2.51     5.   -1021.  ]]\n",
            "Linear Regression - Predicted Vibration on the 11th day: [    3.73    12.25 -1021.73]\n",
            "Linear Regression - Vibration Error for the 11th day: [[6.24 7.25 0.73]]\n",
            "\n",
            "Linear Regression - Actual Temperature on the 11th day: 31.5\n",
            "Linear Regression - Predicted Temperature on the 11th day: 29.870298427069844\n",
            "Linear Regression - Temperature Error for the 11th day: 1.6297015729301556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create random forest models\n",
        "model_vibration = RandomForestRegressor()\n",
        "model_temperature = RandomForestRegressor()"
      ],
      "metadata": {
        "id": "FCvoAvLxzAU-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vibration.fit(X, y_vibration)\n",
        "model_temperature.fit(X, y_temperature)\n",
        "\n",
        "# Predict the 11th day values\n",
        "vibration_11th_day_prediction = model_vibration.predict(X.iloc[[-1]])\n",
        "temperature_11th_day_prediction = model_temperature.predict(X.iloc[[-1]])\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-2.51,5,-1021]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 31.5  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error = np.abs(vibration_11th_day_prediction[0] - actual_vibration_11th_day)\n",
        "temperature_error = np.abs(temperature_11th_day_prediction[0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"Linear Regression - Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"Linear Regression - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction[0])\n",
        "print(\"Linear Regression - Vibration Error for the 11th day:\", vibration_error)\n",
        "\n",
        "print(\"\\nLinear Regression - Actual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"Linear Regression - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction[0])\n",
        "print(\"Linear Regression - Temperature Error for the 11th day:\", temperature_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZSzsFtlzC-V",
        "outputId": "77fe1805-a5ef-4ee9-8afb-b6586ac9ddf7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression - Actual Vibration on the 11th day: [[   -2.51     5.   -1021.  ]]\n",
            "Linear Regression - Predicted Vibration on the 11th day: [    2.1663    12.7639 -1021.2221]\n",
            "Linear Regression - Vibration Error for the 11th day: [[4.6763 7.7639 0.2221]]\n",
            "\n",
            "Linear Regression - Actual Temperature on the 11th day: 31.5\n",
            "Linear Regression - Predicted Temperature on the 11th day: 30.48700000000003\n",
            "Linear Regression - Temperature Error for the 11th day: 1.0129999999999697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Create XGBoost models\n",
        "model_vibration = XGBRegressor()\n",
        "model_temperature = XGBRegressor()\n",
        "\n",
        "# ... (rest of the code is similar to Linear Regression)\n"
      ],
      "metadata": {
        "id": "KFjwil_WzGt2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vibration.fit(X, y_vibration)\n",
        "model_temperature.fit(X, y_temperature)\n",
        "\n",
        "# Predict the 11th day values\n",
        "vibration_11th_day_prediction = model_vibration.predict(X.iloc[[-1]])\n",
        "temperature_11th_day_prediction = model_temperature.predict(X.iloc[[-1]])\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-2.51,5,-1021]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 31.5  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error = np.abs(vibration_11th_day_prediction[0] - actual_vibration_11th_day)\n",
        "temperature_error = np.abs(temperature_11th_day_prediction[0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"Linear Regression - Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"Linear Regression - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction[0])\n",
        "print(\"Linear Regression - Vibration Error for the 11th day:\", vibration_error)\n",
        "\n",
        "print(\"\\nLinear Regression - Actual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"Linear Regression - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction[0])\n",
        "print(\"Linear Regression - Temperature Error for the 11th day:\", temperature_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL0eNtrIzEh1",
        "outputId": "450f5e9d-2dbf-44c0-e528-ca455727016d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression - Actual Vibration on the 11th day: [[   -2.51     5.   -1021.  ]]\n",
            "Linear Regression - Predicted Vibration on the 11th day: [    3.7288156    12.250746  -1021.73065  ]\n",
            "Linear Regression - Vibration Error for the 11th day: [[6.23881556 7.25074577 0.73065186]]\n",
            "\n",
            "Linear Regression - Actual Temperature on the 11th day: 31.5\n",
            "Linear Regression - Predicted Temperature on the 11th day: 31.528843\n",
            "Linear Regression - Temperature Error for the 11th day: 0.028842926025390625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Create LSTM models\n",
        "model_vibration = Sequential()\n",
        "model_temperature = Sequential()\n",
        "\n",
        "# Add LSTM layers to the models\n",
        "model_vibration.add(LSTM(units=50, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "model_vibration.add(Dense(units=X.shape[1]))  # Output layer for vibration\n",
        "\n",
        "model_temperature.add(LSTM(units=50, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "model_temperature.add(Dense(units=1))  # Output layer for temperature\n",
        "\n",
        "# Compile the models\n",
        "model_vibration.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_temperature.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "X_lstm = np.array(X).reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Train the models\n",
        "model_vibration.fit(X_lstm, y_vibration, epochs=100, batch_size=32)\n",
        "model_temperature.fit(X_lstm, y_temperature, epochs=100, batch_size=32)\n",
        "\n",
        "# Predict the 11th day values\n",
        "X_11th_day = np.array(X.iloc[[-1]]).reshape(1, X.shape[1], 1)\n",
        "vibration_11th_day_prediction = model_vibration.predict(X_11th_day)\n",
        "temperature_11th_day_prediction = model_temperature.predict(X_11th_day)\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-10, 18, -1018]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 30.0  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error = np.abs(vibration_11th_day_prediction[0] - actual_vibration_11th_day)\n",
        "temperature_error = np.abs(temperature_11th_day_prediction[0][0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"LSTM - Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"LSTM - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction[0])\n",
        "print(\"LSTM - Vibration Error for the 11th day:\", vibration_error)\n",
        "\n",
        "print(\"\\nLSTM - Actual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"LSTM - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction[0][0])\n",
        "print(\"LSTM - Temperature Error for the 11th day:\", temperature_error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04eDbtICzFht",
        "outputId": "8dcdbc84-90a0-40dd-95a6-2553575abf2b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 340395.6562\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 339101.9375\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 336733.8750\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 332886.5938\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 327397.5312\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 320648.4062\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 313900.5312\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 308710.4375\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 305496.7188\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 303616.5625\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 302324.3438\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 301349.8438\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 300474.1250\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 299628.1875\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 298785.7812\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 297933.8750\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 297062.8750\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 296164.9688\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 295239.7188\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 294302.1562\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 293373.3750\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 292459.3125\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 291552.5625\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 290645.3750\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 289733.4062\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 288815.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 287889.5000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 286956.7812\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 286016.8438\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 285069.7812\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 284115.6250\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 283154.4688\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 282186.1875\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 281210.8438\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 280228.3438\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 279238.6875\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 278241.8438\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 277237.7812\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 276226.4375\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 275207.6875\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 274181.6562\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 273148.0312\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 272106.8125\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 271057.8438\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 270000.9062\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 268936.0625\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 267863.0938\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 266781.7812\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 265692.0312\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 264593.6875\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 263486.6562\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 262370.7812\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 261246.2344\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 260113.0938\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 258969.7969\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 257816.7031\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 256654.0156\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 255481.5312\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 254298.8906\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 253105.7969\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 251901.8594\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 250686.3594\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 249457.7969\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 248214.6719\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 246955.0312\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 245675.7969\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 244356.7812\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 242932.9375\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 241208.9531\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 238613.0938\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 233656.9375\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 224068.2344\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 211264.6406\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 202377.7031\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 198504.0156\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 196288.9688\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 194417.9219\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 192599.9375\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 190775.7656\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 188934.2969\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 187074.9688\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 185199.4062\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 183309.0625\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 181393.5625\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 179393.5469\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 177293.2344\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 175111.1719\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 172916.8281\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 170801.9688\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 168716.0938\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 166537.3438\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 164017.4844\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 160394.8281\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 154519.7188\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 146025.7031\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 136037.8594\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 127332.2656\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 121800.0312\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 118116.4141\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 114446.9844\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3617.7681\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3406.6641\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3203.0120\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3003.9177\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2807.8591\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2612.0310\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2410.6653\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2191.7864\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1932.7444\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1600.1089\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1170.5807\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 691.8250\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 313.1266\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 124.8785\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 55.2570\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 28.5169\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 16.2261\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.8118\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4660\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.0588\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.0037\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.9192\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.5184\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.5676\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 11.8677\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 14.2467\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 16.5566\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 18.6953\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 20.6351\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 22.3828\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 23.9033\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 25.1445\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 26.0775\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 26.6954\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 27.0055\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 27.0239\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 26.7732\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 26.2809\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 25.5770\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 24.6935\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 23.6625\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 22.5146\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 21.2760\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 19.9596\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 18.5551\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 17.0007\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 15.0996\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 12.3786\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.2217\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.9215\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6574\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.6000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.0059\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.1682\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.9432\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.4655\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.1694\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.6476\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.1884\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.0474\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.0598\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.0677\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.0436\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.9980\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.9490\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.9131\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.8982\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.8989\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.8987\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.8853\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.8610\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.8362\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.8185\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.8085\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.8033\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.7994\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7947\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7884\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7805\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7718\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.7633\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7558\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.7499\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7456\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7425\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7398\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7369\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7333\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7292\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7248\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7205\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.7166\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7133\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7105\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.7079\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7056\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7032\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7007\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.6981\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ebd6c97cca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 202ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ebd6c97e830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 185ms/step\n",
            "LSTM - Actual Vibration on the 11th day: [[  -10    18 -1018]]\n",
            "LSTM - Predicted Vibration on the 11th day: [ -78.62479   -35.185978 -441.29868 ]\n",
            "LSTM - Vibration Error for the 11th day: [[ 68.62478638  53.18597794 576.70132446]]\n",
            "\n",
            "LSTM - Actual Temperature on the 11th day: 30.0\n",
            "LSTM - Predicted Temperature on the 11th day: 29.160715\n",
            "LSTM - Temperature Error for the 11th day: 0.8392848968505859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU\n",
        "\n",
        "from keras.layers import GRU\n",
        "\n",
        "# Create GRU models\n",
        "model_vibration = Sequential()\n",
        "model_temperature = Sequential()\n",
        "\n",
        "# Add GRU layers to the models\n",
        "model_vibration.add(GRU(units=50, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "model_vibration.add(Dense(units=X.shape[1]))  # Output layer for vibration\n",
        "\n",
        "model_temperature.add(GRU(units=50, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "model_temperature.add(Dense(units=1))  # Output layer for temperature\n",
        "\n",
        "# Compile the models\n",
        "model_vibration.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_temperature.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Reshape the input data for GRU\n",
        "X_gru = np.array(X).reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Train the models\n",
        "model_vibration.fit(X_gru, y_vibration, epochs=100, batch_size=32)\n",
        "model_temperature.fit(X_gru, y_temperature, epochs=100, batch_size=32)\n",
        "\n",
        "# Predict the 11th day values\n",
        "X_11th_day_gru = np.array(X.iloc[[-1]]).reshape(1, X.shape[1], 1)\n",
        "vibration_11th_day_prediction_gru = model_vibration.predict(X_11th_day_gru)\n",
        "temperature_11th_day_prediction_gru = model_temperature.predict(X_11th_day_gru)\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-2.51,5,-1021]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 30.0  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error_gru = np.abs(vibration_11th_day_prediction_gru[0] - actual_vibration_11th_day)\n",
        "temperature_error_gru = np.abs(temperature_11th_day_prediction_gru[0][0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"GRU - Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"GRU - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction_gru[0])\n",
        "print(\"GRU - Vibration Error for the 11th day:\", vibration_error_gru)\n",
        "\n",
        "print(\"\\nGRU - Actual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"GRU - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction_gru[0][0])\n",
        "print(\"GRU - Temperature Error for the 11th day:\", temperature_error_gru)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfinMBgzzOkX",
        "outputId": "bb7fef96-afa8-4eec-9988-5d806c28fb5c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 350559.6562\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 348553.7188\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 346532.0000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 344498.7812\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 342463.5938\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 340428.5625\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 338395.7188\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 336370.8125\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 334359.2812\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 332360.5312\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 330372.5625\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 328394.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 326424.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 324462.1875\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 322509.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 320564.1875\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 318626.0625\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 316693.1250\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 314763.5312\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 312836.3438\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 310913.3438\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 308998.7812\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 307093.4688\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 305193.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 303293.4062\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 301392.5312\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 299484.6250\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 297546.4375\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 295520.5312\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 293275.5938\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 290512.0625\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 286599.1250\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 280462.1250\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 271214.5000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 260259.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 251501.2344\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 246426.4688\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 243349.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 240950.7656\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 238754.6406\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 236616.2031\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 234490.5312\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 232360.5469\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 230220.0781\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 228061.4688\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 225843.7031\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 223453.1719\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 220610.6406\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 216742.5312\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 211066.2656\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 203521.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 195399.5156\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 187180.4688\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 178734.7969\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 172239.8281\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 168340.9844\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 165572.3438\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 163146.2031\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 160809.0625\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 158489.8438\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 156177.2344\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 153869.8281\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 151568.6562\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 149275.1094\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 146990.6406\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 144716.5000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 142453.7031\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 140203.1719\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 137965.5312\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 135741.4219\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 133531.2812\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 131335.4375\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 129154.1719\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 126988.8203\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 124839.8281\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 122707.2266\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 120591.2656\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 118489.7344\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 116391.4844\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 114254.7656\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 111990.5859\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 109321.5078\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 105711.4609\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 101003.7812\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 96464.7656\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 93373.8828\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 91112.0469\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 89023.3438\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 86957.8359\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 84892.7969\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 82827.7500\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 80765.6094\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 78709.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 76658.6016\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 74610.5156\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 72558.7266\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 70506.3281\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 68454.2109\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 66396.8203\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 64358.0898\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 70.5014\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.8707\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 46.6561\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 24.9568\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.8608\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 14.8876\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 25.3532\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 20.5717\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.0935\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.8411\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.1841\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 16.2325\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 12.5283\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.1490\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.9150\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.3864\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 11.0080\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.8065\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.4139\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5184\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.8173\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.9697\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.8537\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.7857\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4624\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.0871\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.3349\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5267\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.5063\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4952\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5693\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.3685\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.6493\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.0359\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3898\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4469\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.9171\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.0782\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.7277\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3248\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3363\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6319\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.7176\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4655\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2370\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2949\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4696\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4735\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2992\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 4.1820\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.2462\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3433\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3009\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.1793\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.1445\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.2072\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2378\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1772\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1092\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1161\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.1559\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.1438\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0894\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0664\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0875\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0958\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0649\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0339\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0362\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0471\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0322\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.0053\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9974\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0033\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9964\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9760\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9638\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9639\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.9590\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9448\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9341\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9306\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9249\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9124\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9017\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.8971\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.8910\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.8809\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.8717\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8658\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8590\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8494\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8403\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8333\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8256\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8162\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8080\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.8012\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7934\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7843\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "GRU - Actual Vibration on the 11th day: [[   -2.51     5.   -1021.  ]]\n",
            "GRU - Predicted Vibration on the 11th day: [  49.639545   33.968853 -576.4315  ]\n",
            "GRU - Vibration Error for the 11th day: [[ 52.14954544  28.968853   444.56848145]]\n",
            "\n",
            "GRU - Actual Temperature on the 11th day: 30.0\n",
            "GRU - Predicted Temperature on the 11th day: 30.962362\n",
            "GRU - Temperature Error for the 11th day: 0.9623622894287109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, LSTM, Dense\n",
        "\n",
        "# Create BiLSTM models\n",
        "model_vibration_bilstm = Sequential()\n",
        "model_temperature_bilstm = Sequential()\n",
        "\n",
        "# Add BiLSTM layers to the models\n",
        "model_vibration_bilstm.add(Bidirectional(LSTM(units=50, activation='relu'), input_shape=(X.shape[1], 1)))\n",
        "model_vibration_bilstm.add(Dense(units=X.shape[1]))  # Output layer for vibration\n",
        "\n",
        "model_temperature_bilstm.add(Bidirectional(LSTM(units=50, activation='relu'), input_shape=(X.shape[1], 1)))\n",
        "model_temperature_bilstm.add(Dense(units=1))  # Output layer for temperature\n",
        "\n",
        "# Compile the models\n",
        "model_vibration_bilstm.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_temperature_bilstm.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Reshape the input data for BiLSTM\n",
        "X_bilstm = np.array(X).reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Train the models\n",
        "model_vibration_bilstm.fit(X_bilstm, y_vibration, epochs=100, batch_size=32)\n",
        "model_temperature_bilstm.fit(X_bilstm, y_temperature, epochs=100, batch_size=32)\n",
        "\n",
        "# Predict the 11th day values\n",
        "X_11th_day_bilstm = np.array(X.iloc[[-1]]).reshape(1, X.shape[1], 1)\n",
        "vibration_11th_day_prediction_bilstm = model_vibration_bilstm.predict(X_11th_day_bilstm)\n",
        "temperature_11th_day_prediction_bilstm = model_temperature_bilstm.predict(X_11th_day_bilstm)\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day_bilstm = np.array([[-10, 18, -1018]])  # Replace with the actual values\n",
        "actual_temperature_11th_day_bilstm = 30.0  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error_bilstm = np.abs(vibration_11th_day_prediction_bilstm[0] - actual_vibration_11th_day_bilstm)\n",
        "temperature_error_bilstm = np.abs(temperature_11th_day_prediction_bilstm[0][0] - actual_temperature_11th_day_bilstm)\n",
        "\n",
        "# Display results\n",
        "print(\"BiLSTM - Actual Vibration on the 11th day:\", actual_vibration_11th_day_bilstm)\n",
        "print(\"BiLSTM - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction_bilstm[0])\n",
        "print(\"BiLSTM - Vibration Error for the 11th day:\", vibration_error_bilstm)\n",
        "\n",
        "print(\"\\nBiLSTM - Actual Temperature on the 11th day:\", actual_temperature_11th_day_bilstm)\n",
        "print(\"BiLSTM - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction_bilstm[0][0])\n",
        "print(\"BiLSTM - Temperature Error for the 11th day:\", temperature_error_bilstm)\n"
      ],
      "metadata": {
        "id": "I3BlpbUf0VIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de65de2f-09c2-42b5-c143-3b5731611190"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 328101.1562\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 321515.6562\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 315288.4688\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 306755.5312\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 300775.0625\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 292858.3438\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 282510.7188\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 274501.9688\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 269504.4688\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 266045.7500\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 262089.8281\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 257541.7344\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 252561.1406\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 248004.4375\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 244501.9062\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 240400.2969\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 235145.3281\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 228215.5938\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 218872.2031\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 210894.4219\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 205571.8906\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 202597.5312\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 199106.2344\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 194649.9062\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 189775.8438\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 184496.5938\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 178487.5938\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 171702.3594\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 164574.2344\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 157667.3594\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 151486.5000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 146035.4844\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 140814.9844\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 135476.7031\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 129916.6250\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 124267.7969\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 118552.3984\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 112479.4141\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 105581.1016\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 99103.4297\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 93105.1953\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 87491.7344\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 81852.2969\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 75969.6016\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 69215.3672\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 60739.6172\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 52426.1484\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 46439.3008\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 41653.4648\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 37162.6406\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 32906.9297\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 28822.2871\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 24766.8008\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 20745.2090\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 16941.0547\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 13582.8857\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 10725.1494\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8288.5400\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6217.6792\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4487.5420\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3068.1067\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1935.8374\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1081.0099\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 499.7275\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 169.5947\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 134.0921\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 160.7762\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 324.2826\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 610.1785\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 912.9528\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1197.9821\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1433.3864\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1589.3478\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1728.5547\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1833.9216\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1832.8674\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1762.1119\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1641.0448\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1513.5696\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1376.2202\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1196.6442\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1008.7196\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 859.8888\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 699.6420\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 564.2551\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 445.9051\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 348.1960\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 275.2901\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 217.1975\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 179.2498\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 143.7568\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 133.6003\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 119.2944\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 121.3547\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 118.2519\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 125.9463\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 129.1141\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 136.1660\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 144.2553\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 150.0186\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 54.7991\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1344\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 24.0045\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 23.5585\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 14.8898\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.3956\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.4677\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.0343\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.7571\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 11.0329\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.1909\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8006\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.1303\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5022\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.6280\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.0898\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.5415\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.7468\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.6370\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.2821\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.8338\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4547\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.2557\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.2554\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.3957\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5701\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6774\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6723\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5617\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3913\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.2255\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1250\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1490\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2173\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2548\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.2583\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.2266\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.1712\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.1139\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0767\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0683\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0817\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.1003\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.1076\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0957\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0678\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0354\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.0127\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.0061\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0095\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0125\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0064\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9892\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9657\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9418\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9207\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.8995\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8713\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8338\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8065\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8316\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.7912\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7699\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.7717\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.7641\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7384\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7088\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.6935\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6899\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6762\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6497\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6390\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.6195\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6003\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.5836\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.5663\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.5480\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.5270\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.5071\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.4921\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.4738\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.4507\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.4275\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.4069\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3826\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.3598\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.3345\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.3062\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.2677\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3295\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3467\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.1966\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.2592\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.2012\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1423\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.1661\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.1380\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.0811\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.0823\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0699\n",
            "1/1 [==============================] - 0s 281ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "BiLSTM - Actual Vibration on the 11th day: [[  -10    18 -1018]]\n",
            "BiLSTM - Predicted Vibration on the 11th day: [   -6.9109387    21.442013  -1005.5147   ]\n",
            "BiLSTM - Vibration Error for the 11th day: [[ 3.08906126  3.44201279 12.48529053]]\n",
            "\n",
            "BiLSTM - Actual Temperature on the 11th day: 30.0\n",
            "BiLSTM - Predicted Temperature on the 11th day: 30.9816\n",
            "BiLSTM - Temperature Error for the 11th day: 0.9815998077392578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow keras\n"
      ],
      "metadata": {
        "id": "clkmdMibpatd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3VMCesY-xILM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Create Stacked LSTM models\n",
        "model_vibration = Sequential()\n",
        "model_temperature = Sequential()\n",
        "\n",
        "# Add Stacked LSTM layers to the models\n",
        "model_vibration.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(X.shape[1], 1)))\n",
        "model_vibration.add(LSTM(units=50, activation='relu'))\n",
        "model_vibration.add(Dense(units=X.shape[1]))  # Output layer for vibration\n",
        "\n",
        "model_temperature.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(X.shape[1], 1)))\n",
        "model_temperature.add(LSTM(units=50, activation='relu'))\n",
        "model_temperature.add(Dense(units=1))  # Output layer for temperature\n",
        "\n",
        "# Compile the models\n",
        "model_vibration.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_temperature.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Reshape the input data for Stacked LSTM\n",
        "X_stacked_lstm = np.array(X).reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Train the models\n",
        "model_vibration.fit(X_stacked_lstm, y_vibration, epochs=100, batch_size=32)\n",
        "model_temperature.fit(X_stacked_lstm, y_temperature, epochs=100, batch_size=32)\n",
        "\n",
        "# Predict the 11th day values\n",
        "X_11th_day_stacked_lstm = np.array(X.iloc[[-1]]).reshape(1, X.shape[1], 1)\n",
        "vibration_11th_day_prediction_stacked_lstm = model_vibration.predict(X_11th_day_stacked_lstm)\n",
        "temperature_11th_day_prediction_stacked_lstm = model_temperature.predict(X_11th_day_stacked_lstm)\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-10, 18, -1018]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 30.0  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error_stacked_lstm = np.abs(vibration_11th_day_prediction_stacked_lstm[0] - actual_vibration_11th_day)\n",
        "temperature_error_stacked_lstm = np.abs(temperature_11th_day_prediction_stacked_lstm[0][0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"Stacked LSTM - Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"Stacked LSTM - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction_stacked_lstm[0])\n",
        "print(\"Stacked LSTM - Vibration Error for the 11th day:\", vibration_error_stacked_lstm)\n",
        "\n",
        "print(\"\\nStacked LSTM - Actual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"Stacked LSTM - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction_stacked_lstm[0][0])\n",
        "print(\"Stacked LSTM - Temperature Error for the 11th day:\", temperature_error_stacked_lstm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9I9DamdxKfA",
        "outputId": "2d209986-6e82-4180-975b-acc7674ef47f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 344180.9375\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 341833.6250\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 339399.8125\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 337281.8750\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 335624.8750\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 334321.0312\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 333216.4688\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 332205.8750\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 331226.9375\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 330241.6250\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 329225.9375\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 328157.5938\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 327006.1875\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 325722.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 324227.5312\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 322434.6875\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 320443.4688\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 318432.8125\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 316711.8750\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 315308.8750\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 314113.4062\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 313003.1875\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 311884.0938\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 310660.1562\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 309185.6562\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 307219.6250\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 304419.4375\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 301787.7188\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 299800.9688\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 298297.3438\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 295303.8438\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 292199.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 289047.6562\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 284896.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 279733.2188\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 275171.3125\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 271443.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 268151.5312\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 264696.7188\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 259727.0312\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 255396.1094\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 251115.6719\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 246571.6719\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 241310.2344\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 234364.2188\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 226725.8906\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 219198.7344\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 211876.3906\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 204681.2969\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 197078.2656\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 189160.9062\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 181339.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 172826.2656\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 163948.3125\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 155749.4531\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 148254.3125\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 140912.2969\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 133477.9688\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 125941.5859\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 118341.4453\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 110704.0312\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 103001.6094\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 95327.2969\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 87716.6328\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 80114.2031\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 72606.7656\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 65338.0664\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 58231.9258\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 51154.2266\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 44290.4570\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 37545.2070\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 30547.1211\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 23481.9961\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 18413.7637\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 15449.5830\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 13472.8623\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 12299.4102\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11716.4502\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11529.4082\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11363.6035\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11073.8271\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10661.0947\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 10207.7607\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9586.3232\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8970.2783\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8228.3809\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7054.8970\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6485.5132\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5887.8540\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5327.7505\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4784.3330\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4380.2344\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4164.7959\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3972.2837\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3665.1604\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3322.9268\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2980.4375\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2797.2756\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2708.3499\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2499.4832\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 257.2071\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 132.5762\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 51.9949\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.5867\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 45.0968\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 44.1613\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 16.7298\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.1179\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.8079\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 12.3838\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 13.6134\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.2960\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.1509\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.6418\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.0705\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.6364\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.4658\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.6260\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.1349\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.9691\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.0723\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.3667\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.7654\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.1830\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.5465\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8024\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.9208\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8955\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.7409\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.4868\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.1717\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.8369\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.5200\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.2508\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.0480\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9203\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8653\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8720\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9227\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.9968\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.0746\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.1399\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.1813\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.1928\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.1742\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.1298\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.0673\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.9960\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9253\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.8631\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.8150\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7837\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7691\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.7687\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.7779\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7918\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 4.8055\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8150\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.8177\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.8127\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.8008\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7835\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7633\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.7424\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7224\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7031\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.6837\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.6659\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.6588\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.6619\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.6629\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.6581\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.6479\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6343\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6193\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.6052\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5934\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5847\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5794\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5767\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5759\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5757\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.5751\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.5737\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5706\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5655\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5581\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.5475\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5358\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5806\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5303\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5239\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5226\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5166\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4976\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4860\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 4.4695\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.4661\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.4543\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.4407\n",
            "1/1 [==============================] - 0s 331ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "Stacked LSTM - Actual Vibration on the 11th day: [[  -10    18 -1018]]\n",
            "Stacked LSTM - Predicted Vibration on the 11th day: [ -63.143566   12.946047 -941.82904 ]\n",
            "Stacked LSTM - Vibration Error for the 11th day: [[53.14356613  5.05395317 76.17095947]]\n",
            "\n",
            "Stacked LSTM - Actual Temperature on the 11th day: 30.0\n",
            "Stacked LSTM - Predicted Temperature on the 11th day: 29.265474\n",
            "Stacked LSTM - Temperature Error for the 11th day: 0.7345256805419922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, LSTM, Dense\n",
        "\n",
        "# Create Bidirectional LSTM models\n",
        "model_vibration = Sequential()\n",
        "model_temperature = Sequential()\n",
        "\n",
        "# Add Bidirectional LSTM layers to the models\n",
        "model_vibration.add(Bidirectional(LSTM(units=50, activation='relu'), input_shape=(X.shape[1], 1)))\n",
        "model_vibration.add(Dense(units=X.shape[1]))  # Output layer for vibration\n",
        "\n",
        "model_temperature.add(Bidirectional(LSTM(units=50, activation='relu'), input_shape=(X.shape[1], 1)))\n",
        "model_temperature.add(Dense(units=1))  # Output layer for temperature\n",
        "\n",
        "# Compile the models\n",
        "model_vibration.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_temperature.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Reshape the input data for Bidirectional LSTM\n",
        "X_bilstm = np.array(X).reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Train the models\n",
        "model_vibration.fit(X_bilstm, y_vibration, epochs=100, batch_size=32)\n",
        "model_temperature.fit(X_bilstm, y_temperature, epochs=100, batch_size=32)\n",
        "\n",
        "# Predict the 11th day values\n",
        "X_11th_day_bilstm = np.array(X.iloc[[-1]]).reshape(1, X.shape[1], 1)\n",
        "vibration_11th_day_prediction_bilstm = model_vibration.predict(X_11th_day_bilstm)\n",
        "temperature_11th_day_prediction_bilstm = model_temperature.predict(X_11th_day_bilstm)\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-10, 18, -1018]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 30.0  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error_bilstm = np.abs(vibration_11th_day_prediction_bilstm[0] - actual_vibration_11th_day)\n",
        "temperature_error_bilstm = np.abs(temperature_11th_day_prediction_bilstm[0][0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"BiLSTM - Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"BiLSTM - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction_bilstm[0])\n",
        "print(\"BiLSTM - Vibration Error for the 11th day:\", vibration_error_bilstm)\n",
        "\n",
        "print(\"\\nBiLSTM - Actual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"BiLSTM - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction_bilstm[0][0])\n",
        "print(\"BiLSTM - Temperature Error for the 11th day:\", temperature_error_bilstm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjMd43JAzeCz",
        "outputId": "c6d97f96-0cba-4796-b172-9f4365ff36e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 344457.4062\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 336396.7188\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 329936.3438\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 324974.8125\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 321894.3438\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 319725.1562\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 317638.0000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 315339.0625\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 312262.0625\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 305657.5938\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 297282.7812\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 292066.9062\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 288555.0625\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 285355.4688\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 282219.4375\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 279254.3750\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 276465.8125\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 273771.3438\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 271078.4375\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 268298.3750\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 265306.3125\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 261881.7656\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 257736.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 252838.5000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 247810.3281\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 243407.7500\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 239539.5312\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 235631.7344\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 231207.3594\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 226020.2969\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 220211.4062\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 214260.5938\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 205660.3594\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 207445.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 195441.5312\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 189601.9375\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 185189.8125\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 180279.4375\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 175054.3594\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 169877.4062\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 164764.6562\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 159608.7344\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 154376.5625\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 149104.1562\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 143869.2656\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 138736.0312\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 133483.5781\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 128118.6094\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 122776.7188\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 117530.1328\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 112344.4297\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 107154.6172\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 101923.3672\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 96632.7266\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 91269.7812\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 85827.8672\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 80306.3828\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 74713.4297\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 69038.1094\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 63392.7656\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 57850.5664\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 52403.2969\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 46976.9844\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 41375.2266\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 35506.3828\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 29388.3613\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 19045.5840\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 14767.7510\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 11556.8770\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8666.6211\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5966.9790\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3550.6021\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1623.8796\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 472.9756\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 118.7555\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 305.2647\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 772.7705\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1346.7213\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1917.2177\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2405.2898\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2764.6824\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2975.5066\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3034.4426\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2945.4875\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2694.0803\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2205.6433\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1523.7218\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1049.1422\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 780.2256\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 570.8073\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 407.0779\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 288.2291\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 210.0436\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 164.7754\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 143.2753\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 136.9417\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 138.9463\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 144.6715\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 151.5397\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 158.4977\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 8606.4795\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6758.7798\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5964.5854\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5390.7354\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4793.7300\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4139.8447\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3405.4988\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2593.5730\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1632.9641\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 845.1354\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 357.6582\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 124.0265\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 33.9219\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 16.8763\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 35.2763\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 64.0329\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 89.7330\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 107.1164\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 114.9803\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 114.0018\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 105.4246\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 90.8073\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 71.4630\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 49.3541\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 29.0010\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 17.7339\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 21.2743\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 34.1683\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 40.9337\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 36.3482\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 25.4657\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 15.7718\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 11.4635\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.1728\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.1688\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 17.8986\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 18.9939\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 18.1184\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 15.6121\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.2397\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.9991\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8588\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3630\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.2698\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.6099\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.2987\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.8328\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.4833\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.9394\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.8044\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3197\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3836\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7229\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.0620\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.2178\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.1271\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.8296\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4298\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0508\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7916\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6980\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7544\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8987\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.0512\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1458\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.1520\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0776\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.9578\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.8371\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7511\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7169\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.7308\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7744\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.8233\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8560\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.8609\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8378\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.7958\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7488\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7099\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.6874\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.6828\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.6912\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7044\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7139\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7144\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7047\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.6880\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6694\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.6542\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6457\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6441\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.6473\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.6517\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.6540\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6523\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.6465\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6383\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6301\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6237\n",
            "1/1 [==============================] - 0s 400ms/step\n",
            "1/1 [==============================] - 0s 334ms/step\n",
            "BiLSTM - Actual Vibration on the 11th day: [[  -10    18 -1018]]\n",
            "BiLSTM - Predicted Vibration on the 11th day: [  -21.364876    28.046558 -1001.32794 ]\n",
            "BiLSTM - Vibration Error for the 11th day: [[11.36487579 10.04655838 16.67205811]]\n",
            "\n",
            "BiLSTM - Actual Temperature on the 11th day: 30.0\n",
            "BiLSTM - Predicted Temperature on the 11th day: 30.086079\n",
            "BiLSTM - Temperature Error for the 11th day: 0.08607864379882812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Reshape, Conv1D, LSTM, Dense\n",
        "\n",
        "# Assume X is a 2D array with shape (samples, features)\n",
        "\n",
        "# Reshape X to include a time step dimension\n",
        "X_reshaped = np.array(X).reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Create ConvLSTM models\n",
        "model_vibration = Sequential()\n",
        "model_temperature = Sequential()\n",
        "\n",
        "# Add Convolutional layer followed by LSTM layers to the models\n",
        "model_vibration.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "model_vibration.add(LSTM(units=50, activation='relu'))\n",
        "model_vibration.add(Dense(units=X.shape[1]))  # Output layer for vibration\n",
        "\n",
        "model_temperature.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "model_temperature.add(LSTM(units=50, activation='relu'))\n",
        "model_temperature.add(Dense(units=1))  # Output layer for temperature\n",
        "\n",
        "# Compile the models\n",
        "model_vibration.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_temperature.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the models\n",
        "model_vibration.fit(X_reshaped, y_vibration, epochs=100, batch_size=32)\n",
        "model_temperature.fit(X_reshaped, y_temperature, epochs=100, batch_size=32)\n",
        "\n",
        "# Predict the 11th day values\n",
        "X_11th_day_reshaped = np.array(X.iloc[[-1]]).reshape(1, X.shape[1], 1)\n",
        "vibration_11th_day_prediction = model_vibration.predict(X_11th_day_reshaped)\n",
        "temperature_11th_day_prediction = model_temperature.predict(X_11th_day_reshaped)\n",
        "\n",
        "# Actual values for the 11th day\n",
        "actual_vibration_11th_day = np.array([[-10, 18, -1018]])  # Replace with the actual values\n",
        "actual_temperature_11th_day = 30.0  # Replace with the actual value\n",
        "\n",
        "# Calculate errors\n",
        "vibration_error = np.abs(vibration_11th_day_prediction[0] - actual_vibration_11th_day)\n",
        "temperature_error = np.abs(temperature_11th_day_prediction[0][0] - actual_temperature_11th_day)\n",
        "\n",
        "# Display results\n",
        "print(\"ConvLSTM - Actual Vibration on the 11th day:\", actual_vibration_11th_day)\n",
        "print(\"ConvLSTM - Predicted Vibration on the 11th day:\", vibration_11th_day_prediction[0])\n",
        "print(\"ConvLSTM - Vibration Error for the 11th day:\", vibration_error)\n",
        "\n",
        "print(\"\\nConvLSTM - Actual Temperature on the 11th day:\", actual_temperature_11th_day)\n",
        "print(\"ConvLSTM - Predicted Temperature on the 11th day:\", temperature_11th_day_prediction[0][0])\n",
        "print(\"ConvLSTM - Temperature Error for the 11th day:\", temperature_error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0TYtfOS3E3l",
        "outputId": "f8132e08-2f8b-4c19-96ea-88709ec9261a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 6s 6s/step - loss: 319261.4062\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 315746.6562\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 312202.1250\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 308563.0625\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 304519.4062\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 299575.1250\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 295808.1875\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 292951.7812\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 290043.5000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 287053.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 283989.9375\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 280864.3750\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 277680.9688\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 274445.5938\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 271157.9062\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 267817.0938\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 264424.1875\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 260979.6562\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 257483.1406\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 253938.5625\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 250349.2969\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 246715.5781\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 243032.0312\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 239297.5938\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 235510.5938\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 231670.5938\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 227777.4375\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 223831.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 219831.3125\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 215778.5625\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 211673.0312\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 207514.2969\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 203292.6875\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 198893.4375\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 193010.7969\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 179250.8281\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 170553.3594\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 166293.2031\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 162045.6406\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 157740.4688\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 153387.9219\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 148996.5156\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 144572.7031\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 140122.2656\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 135652.8125\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 131166.6562\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 126674.4844\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 122181.1328\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 117692.5938\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 113214.2500\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 108751.0859\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 104308.2109\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 99891.3359\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 95506.7188\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 91160.8516\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 86860.2188\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 82610.8438\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 78418.4219\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 74288.6641\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 70227.5312\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 66241.4922\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 62337.0508\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 58520.5078\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 54797.5898\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 51173.6914\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 47654.1328\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 44244.4844\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 40950.1523\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 37776.1328\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 34726.6641\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 31805.4590\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 29015.9551\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 26361.4492\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 23844.7207\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 21467.6758\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 19231.3750\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 17136.3574\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 15182.7412\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 13370.0117\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 11696.7129\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 10160.4961\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 8758.4355\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 7487.0908\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6342.3652\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5319.2969\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 4412.2920\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3615.3779\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2922.2307\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2325.9922\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1819.3770\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1394.9894\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1045.4065\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 763.1064\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 540.5182\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 370.2708\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 245.3228\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 158.9066\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 104.5574\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 76.2907\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 68.6855\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 849.0881\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 121.4964\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 21.7444\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.2517\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 28.3501\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 54.5462\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 66.5470\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 61.8949\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 46.5746\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 28.3831\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 13.6234\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.1183\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.1067\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.6201\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 11.0866\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 14.1492\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 15.7918\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 15.6248\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 13.8658\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 11.1649\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 8.3446\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.1304\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.9675\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.9318\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.7553\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.9551\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.0131\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.5440\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.4002\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.6833\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.6709\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.6979\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.0370\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8155\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9970\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.4159\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.8545\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.1262\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.1358\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.8995\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.5213\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.1424\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.8839\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.8045\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.8881\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.0602\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.2254\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.3076\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.2767\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.1534\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.9938\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.8599\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7937\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8032\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 4.8653\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 4.9383\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.9835\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9812\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9359\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.8703\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.8125\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.7829\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.7865\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 4.8125\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.8425\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 4.8593\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.8549\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 4.8328\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.8042\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 4.7815\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 4.7727\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7776\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7899\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 4.8007\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 4.8036\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.7972\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.7850\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 4.7728\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.7655\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.7648\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7688\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 4.7736\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 4.7756\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 4.7733\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7678\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7616\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7572\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7557\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.7567\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.7583\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7588\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.7573\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7543\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7509\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7483\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 4.7471\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.7469\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 4.7470\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7466\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7452\n",
            "1/1 [==============================] - 0s 375ms/step\n",
            "1/1 [==============================] - 0s 333ms/step\n",
            "ConvLSTM - Actual Vibration on the 11th day: [[  -10    18 -1018]]\n",
            "ConvLSTM - Predicted Vibration on the 11th day: [   -9.295276    25.330458 -1023.9189  ]\n",
            "ConvLSTM - Vibration Error for the 11th day: [[0.70472431 7.33045769 5.91888428]]\n",
            "\n",
            "ConvLSTM - Actual Temperature on the 11th day: 30.0\n",
            "ConvLSTM - Predicted Temperature on the 11th day: 29.015177\n",
            "ConvLSTM - Temperature Error for the 11th day: 0.9848232269287109\n"
          ]
        }
      ]
    }
  ]
}